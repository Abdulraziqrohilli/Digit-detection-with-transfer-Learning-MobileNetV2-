# MNIST Digit Classification using MobileNetV2 (Transfer Learning)

## Table of Contents

- [Overview](#overview)
- [Dataset](#dataset)
- [Approach](#approach)
- [Model Architecture](#model-architecture)
- [Training Strategy](#training-strategy)
- [Results](#results)
- [Visualizations](#visualizations)


---

## Overview

This project performs **handwritten digit classification** on the **MNIST dataset** using **Transfer Learning with MobileNetV2**.

Although MNIST images are grayscale and low-resolution (28×28), this project adapts them to work with a **pretrained ImageNet model**, demonstrating how transfer learning can be applied even to non-natural image datasets.

The model is trained and fine-tuned using **TensorFlow and Keras**.

---

## Dataset

- Dataset: **MNIST**
- Total images: **70,000**
  - Training: 60,000
  - Testing: 10,000
- Classes: Digits **0–9**
- Original image size: **28×28 grayscale**

### Preprocessing Steps

- Converted grayscale images to **3-channel RGB**
- Normalized pixel values to range **[0, 1]**
- Resized images to **32×32**
- Applied **one-hot encoding** to labels

---

## Approach

1. Load MNIST dataset
2. Convert grayscale images to RGB
3. Resize images to match CNN input
4. Use **MobileNetV2 pretrained on ImageNet** as a feature extractor
5. Add custom classification head
6. Train with frozen base model
7. Fine-tune deeper layers of MobileNetV2
8. Evaluate on test set
9. Visualize predictions
10. Save trained model

---

## Model Architecture

- Base Model: **MobileNetV2**
  - Pretrained on ImageNet
  - `include_top=False`
- Custom Head:
  - Global Average Pooling
  - Dense layer with **Softmax (10 classes)**


## Training Strategy

The training process was carried out in two phases using transfer learning:

**Phase 1: Feature Extraction**
- The MobileNetV2 base model was frozen
- Only the custom classification head was trained
- Optimizer: Adam
- Loss function: Categorical Crossentropy
- Epochs: 10

**Phase 2: Fine-Tuning**
- The base model was partially unfrozen
- First 100 layers remained frozen
- Learning rate reduced to `1e-5`
- Epochs: 5

This strategy helped improve generalization while preventing overfitting.

- Convert this into **academic report format**
- Write a **LinkedIn / portfolio description**
- Optimize README for **recruiters**

## Results

The final model performance on the MNIST test dataset:

- **Test Accuracy:** 84.13%
- **Test Loss:** 0.56

The fine-tuning phase significantly improved accuracy compared to training with a frozen base model.

## Visualizations

Below are sample predictions generated by the trained model:

- Randomly selected test images
- Displayed with **true labels** and **predicted labels**
- Grayscale visualization for clarity

These visual results demonstrate the model’s ability to correctly classify handwritten digits.
